

- Project 1:  [**BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology**](https://aclanthology.org/2021.blackboxnlp-1.43/) (Gessler and Schneider, 2021) ([Code](https://github.com/lgessler/bert-has-uncommon-sense/tree/master))
  Project supervisor: Aina Garí Soler (aina.gari-soler@inria.fr)
  
Words often have multiple meanings, and some are used more frequently than others (e.g., the noun _duck_ typically refers to an animal but it is also a cricket term). This paper introduces a simple method to investigate how well the BERT model captures less common word meanings. 

- Project 2 : [**Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection**](https://arxiv.org/abs/2205.03302) (Balkir et al. 2022) ([Code](https://github.com/esmab/necessity-sufficiency/tree/main))
  Project supervisor: Célia Nouri (celia.nouri@inria.fr)

Hate speech classifiers often over-predict hate speech when identity terms (e.g., "women", "black", "muslim") are present, leading to the over-targeting of mentions of marginalized communities. This paper introduces a refined explainability framework, using generative models and necessity and sufficiency metrics, to analyze such errors in detection.

- Project 3 : Translation 
- Project 4 :   [**When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?**](https://openreview.net/pdf?id=KRLUvxh8uaX) (Yuksekgonul et al., 2023) ([Code](https://github.com/vinid/neg_clip)) Project supervisor: Matthieu Futeral (matthieu.futeral@inria.fr)

Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. This paper introduces a simple training method to overcome the shortcomings of VLMs when it comes to embed this compositional relationship.

- Project 5 : Speech recognition in a low resource language
- Project 6 : Speech language models
- Project 7 : language emergence

